# Memulai Apache Hive

##  Persyaratan Sistem

Apache Hadoop dan Apache Hive berjalan pada *commodity hardware* atau perangkat keras yang tidak spesifik khusus untuk server, dengan kata lain, perangkat keras yang biasa digunakan oleh pemakai biasa bisa digunakan untuk *stand alone* maupun *clustering*.

Apache Hive dan Apache Hadoop dibuat dengan menggunakan bahasa pemrograman Java sehingga JVM (Java Virtual Machine) diperlukan untuk mengoperasikan Apache Hive dan Apache Hadoop. Informasi tentang kebutuhan JVM untuk masing-masing bisa diperoleh di:

1.  [JVM untuk Apache Hadoop](https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions).
2.  [JVM untuk Apache Hive](https://cwiki.apache.org/confluence/display/Hive/AdminManual+Installation).

Perlu diketahui, dokumentasi untuk Apache Hive bukan merupakan dokumentasi terkini, sehingga untuk keperluan JVM sebaiknya menggunakan persyaratan dari Apache Hadoop:

1.  JDK 8 untuk Apache Hadoop 3.0.x - 3.2.x (mengkompilasi dan menjalankan)
2.  JDK 8 untuk mengkompilasi dan menjalankan Apache Hadoop 3.3.x, dan JRE 11 (*runtime* saja, tidak bisa untuk mengkompilasi kode sumber Apache Hadoop) untuk menjalankan Apache Hadoop 3.3.x
3.  Apache Hadoop yang digunakan adalah versi stabil (3.2.x)

JDK yang bisa digunakan adalah JDK dari [OpenJDK](https://adoptium.net/) maupun dari [Oracle](https://www.oracle.com/java/technologies/downloads/#java8). Versi dari vendor lain seharusnya bisa berjalan juga (misal, Amazon Corretto). 

##  Instalasi dan Konfigurasi Software Prasyarat

### JDK

Petunjuk instalasi untuk JDK:

1.  [Oracle JDK 8](https://docs.oracle.com/javase/8/docs/technotes/guides/install/install_overview.html).
2.  [Adoptium](https://adoptium.net/installation).

### Apache Hadoop

Untuk keperluan instalasi, siapkan direktori baru yang kosong. Kita akan menggunakan direktori ini sebagai dasar dari semua software yang kita gunakan:

```bash
$ mkdir -p $HOME/software/bigdata-dev-tools
$ mkdir -p $HOME/master
$ mkdir -p $HOME/env
$ mkdir -p $HOME/training
```

* Baris 1 digunakan sebagai tempat instalasi semua software yang digunakan
* Baris 2 digunakan untuu menyimpan semua master software
* Baris 3 digunakan untuk menyimpan semua file-file yang mengatur *environment variables*
* Baris 4 digunakan untuk menyimpan semua file yang dibuat saat training.

**Catatan:** Perintah-perintah yang dikerjakan di shell / CLI dilakukan pada kondisi keaktifan user tertentu. Jika diawali dengan **$**, maka perintah tersebut dikerjakan oleh user biasa. Jika diawali dengan **#**, maka perintah tersebut dikerjakan oleh *superuser* / *root*.

Setelah membuat direktori, masukkan semua file master di direktori `$HOME/master` kemudian masuk ke direktori tempat software akan di-install:

```bash
$ cd $HOME/software/bigdata-dev-tools
```

Pada posisi ini, semua direktori sudah tersedia dan semua software yang dibutuhkan sudah berada di `$HOME/master`. 


HDFS akan dibuat pada direktori `/tmp/hadoop-${user.name}`. Semua direktori dan file untuk data maupun metadata akan berada pada direktori tersebut. Jika akan mengubah, gunakan opsi **hadoop.tmp.dir** pada file `core-default.xml`. Informasi lengkap bisa dilihat di https://hadoop.apache.org/docs/r3.2.3/hadoop-project-dist/hadoop-common/core-default.xml

Direktori lain (untuk metadata serta data) menyesuaikan pada konfigurasi di atas. Jika ingin mengubah, konfigurasi bisa diatur di `hdfs-default.xml`. Informasi lengkap bisa dilihat pada https://hadoop.apache.org/docs/r3.2.3/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml. Untuk mengubah konfigurasi, atur:

1.  dfs.namenode.name.dir
2.  dfs.datanode.data.dir

Berikut ini adalah contoh konfigurasi di `core-default.xml`:

```xml
<configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/bpdp/software/big-data-dev-tools/hadoop-3.2.3/data-hdfs/</value>
    </property>
</configuration>
```

Untuk Apache Hadoop, diperlukan instalasi `ssh` dan `pdsh`. Install dengan menggunakan perintah:

```bash
$ sudo apt install ssh pdsh
```

Untuk instalasi Apache Hadoop, ekstrak distribusi Apache Hadoop yang sudah diunduh dan kemudian atur *environment variables*:

```bash
$ tar -xvf ~/master/hadoop-3.2.3.tar.g
...
...
...
...
...
hadoop-3.2.3/sbin/stop-balancer.sh
hadoop-3.2.3/sbin/workers.sh
hadoop-3.2.3/sbin/start-balancer.sh
hadoop-3.2.3/sbin/start-all.cmd
hadoop-3.2.3/sbin/hadoop-daemon.sh
hadoop-3.2.3/sbin/stop-yarn.sh
hadoop-3.2.3/sbin/start-all.sh
$ 
```

Setelah itu, buat file yang berisi *environment variables* dengan nama `$HOME/env/hadoop` sebagai berikut:

```bash
export HADOOP_HOME=$HOME/software/big-data-dev-tools/hadoop-3.2.3
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
export PDSH_RCMD_TYPE=ssh
```

Setelah itu, uji instalasi dengan:

```bash
$ source ~/env/hadoop
```

Jalankan Apache Hadoop:

```bash
$ hadoop
Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]
 or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]
  where CLASSNAME is a user-provided Java class

  OPTIONS is none or any of:

buildpaths                       attempt to add class files from build tree
--config dir                     Hadoop config directory
--debug                          turn on shell script debug mode
--help                           usage information
hostnames list[,of,host,names]   hosts to use in slave mode
hosts filename                   list of hosts to use in slave mode
loglevel level                   set the log4j level for this command
workers                          turn on worker mode

  SUBCOMMAND is one of:


    Admin Commands:

daemonlog     get/set the log level for each daemon

    Client Commands:

archive       create a Hadoop archive
checknative   check native Hadoop and compression libraries availability
classpath     prints the class path needed to get the Hadoop jar and the required libraries
conftest      validate configuration XML files
credential    interact with credential providers
distch        distributed metadata changer
distcp        copy file or directories recursively
dtutil        operations related to delegation tokens
envvars       display computed Hadoop environment variables
fs            run a generic filesystem user client
gridmix       submit a mix of synthetic job, modeling a profiled from production load
jar <jar>     run a jar file. NOTE: please use "yarn jar" to launch YARN applications, not this command.
jnipath       prints the java.library.path
kdiag         Diagnose Kerberos Problems
kerbname      show auth_to_local principal conversion
key           manage keys via the KeyProvider
rumenfolder   scale a rumen input trace
rumentrace    convert logs into a rumen trace
s3guard       manage metadata on S3
trace         view and modify Hadoop tracing settings
version       print the version

    Daemon Commands:

kms           run KMS, the Key Management Server

SUBCOMMAND may print help when invoked w/o parameters or with -h.
$
```

Apache Hadoop bisa dijelankan dalam 3 mode:

1.  *Standalone*
2.  *Pseudo-Distributed*
3.  *Fully-Distributed*

### Standalone

Pada mode ini, Hadoop biasanya hanya digunakan untuk proses kecil dan untuk keperluan eksperimen serta *debugging* saja. Dengan menggunakan mode ini, Hadoop (yang dibuat menggunakan Java) akan dijalankan dalam 1 proses Java saja.


Ubah konfigurasi Apache Hadoop untuk mode *pseudo-distributed*

_$HADOOP_HOME/etc/hadoop/core-site.xml_

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```

_$HADOOP_HOME/etc/hadoop/hdfs-site.xml_

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
```

_$HADOOP_HOME/etc/hadoop/hadoop-env.sh_

```bash
export JAVA_HOME=/home/bpdp/software/jdk1.8.0
```

**Catatan**: lokasi JAVA_HOME silakan disesuikan dengan lokasi instalasi JDK.

Setelah itu, atur supaya bisa login ssh tanpa *passphrase*:

```bash
$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
$ chmod 0600 ~/.ssh/authorized_keys
```

Uji dengan perintah berikut:


```bash
$ ssh localhost                                                            (py39-hive)
Linux dellvuan 5.17.0-2-amd64 #1 SMP PREEMPT Debian 5.17.6-1 (2022-05-14) x86_64

The programs included with the Devuan GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Devuan GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
You have new mail.
Last login: Mon May 23 16:43:39 2022
$
```

Setelah itu, format *filesystem*:

```bash
$ hdfs namenode -format 
WARNING: /home/bpdp/software/big-data-dev-tools/hadoop-stable/logs does not exist. Creating.
2022-05-23 17:53:39,064 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = dellvuan/127.0.1.1
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 3.2.3
STARTUP_MSG:   classpath = /home/bpdp/software/big-data-dev-tools/hadoop-stable/etc/hadoop:/home/bpdp/software/big-data-dev-tools/hadoop-stable/share/hadoop/common/lib/paranamer-2.3.jar...
...
...
...
STARTUP_MSG:   build = https://github.com/apache/hadoop -r abe5358143720085498613d399be3bbf01e0f131; compiled by 'ubuntu' on 2022-03-20T01:18Z
STARTUP_MSG:   java = 1.8.0_332
************************************************************/
2022-05-23 18:04:53,863 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2022-05-23 18:04:53,921 INFO namenode.NameNode: createNameNode [-format]
Formatting using clusterid: CID-adddc347-e302-4817-8019-ddcc12113b20
2022-05-23 18:04:54,227 INFO namenode.FSEditLog: Edit logging is async:true
2022-05-23 18:04:54,246 INFO namenode.FSNamesystem: KeyProvider: null
2022-05-23 18:04:54,247 INFO namenode.FSNamesystem: fsLock is fair: true
2022-05-23 18:04:54,249 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2022-05-23 18:04:54,253 INFO namenode.FSNamesystem: fsOwner             = bpdp (auth:SIMPLE)
2022-05-23 18:04:54,253 INFO namenode.FSNamesystem: supergroup          = supergroup
2022-05-23 18:04:54,253 INFO namenode.FSNamesystem: isPermissionEnabled = true
2022-05-23 18:04:54,255 INFO namenode.FSNamesystem: HA Enabled: false
2022-05-23 18:04:54,289 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2022-05-23 18:04:54,297 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2022-05-23 18:04:54,297 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2022-05-23 18:04:54,300 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2022-05-23 18:04:54,300 INFO blockmanagement.BlockManager: The block deletion will start around 2022 May 23 18:04:54
2022-05-23 18:04:54,301 INFO util.GSet: Computing capacity for map BlocksMap
2022-05-23 18:04:54,301 INFO util.GSet: VM type       = 64-bit
2022-05-23 18:04:54,302 INFO util.GSet: 2.0% max memory 1.7 GB = 34.3 MB
2022-05-23 18:04:54,303 INFO util.GSet: capacity      = 2^22 = 4194304 entries
2022-05-23 18:04:54,311 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
2022-05-23 18:04:54,311 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
2022-05-23 18:04:54,315 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2022-05-23 18:04:54,315 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2022-05-23 18:04:54,316 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2022-05-23 18:04:54,316 INFO blockmanagement.BlockManager: defaultReplication         = 1
2022-05-23 18:04:54,316 INFO blockmanagement.BlockManager: maxReplication             = 512
2022-05-23 18:04:54,316 INFO blockmanagement.BlockManager: minReplication             = 1
2022-05-23 18:04:54,316 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
2022-05-23 18:04:54,316 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2022-05-23 18:04:54,316 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
2022-05-23 18:04:54,316 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2022-05-23 18:04:54,333 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
2022-05-23 18:04:54,333 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
2022-05-23 18:04:54,333 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
2022-05-23 18:04:54,333 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
2022-05-23 18:04:54,362 INFO util.GSet: Computing capacity for map INodeMap
2022-05-23 18:04:54,362 INFO util.GSet: VM type       = 64-bit
2022-05-23 18:04:54,362 INFO util.GSet: 1.0% max memory 1.7 GB = 17.2 MB
2022-05-23 18:04:54,362 INFO util.GSet: capacity      = 2^21 = 2097152 entries
2022-05-23 18:04:54,363 INFO namenode.FSDirectory: ACLs enabled? false
2022-05-23 18:04:54,363 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
2022-05-23 18:04:54,363 INFO namenode.FSDirectory: XAttrs enabled? true
2022-05-23 18:04:54,363 INFO namenode.NameNode: Caching file names occurring more than 10 times
2022-05-23 18:04:54,367 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2022-05-23 18:04:54,369 INFO snapshot.SnapshotManager: SkipList is disabled
2022-05-23 18:04:54,373 INFO util.GSet: Computing capacity for map cachedBlocks
2022-05-23 18:04:54,373 INFO util.GSet: VM type       = 64-bit
2022-05-23 18:04:54,373 INFO util.GSet: 0.25% max memory 1.7 GB = 4.3 MB
2022-05-23 18:04:54,373 INFO util.GSet: capacity      = 2^19 = 524288 entries
2022-05-23 18:04:54,378 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2022-05-23 18:04:54,378 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2022-05-23 18:04:54,378 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2022-05-23 18:04:54,381 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
2022-05-23 18:04:54,381 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2022-05-23 18:04:54,382 INFO util.GSet: Computing capacity for map NameNodeRetryCache
2022-05-23 18:04:54,382 INFO util.GSet: VM type       = 64-bit
2022-05-23 18:04:54,382 INFO util.GSet: 0.029999999329447746% max memory 1.7 GB = 527.2 KB
2022-05-23 18:04:54,382 INFO util.GSet: capacity      = 2^16 = 65536 entries
2022-05-23 18:04:54,404 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1171203833-127.0.1.1-1653303894395
2022-05-23 18:04:54,517 INFO common.Storage: Storage directory /home/bpdp/software/big-data-dev-tools/hadoop-3.2.3/data-hdfs/dfs/name has been successfully formatted.
2022-05-23 18:04:54,580 INFO namenode.FSImageFormatProtobuf: Saving image file /home/bpdp/software/big-data-dev-tools/hadoop-3.2.3/data-hdfs/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression
2022-05-23 18:04:54,652 INFO namenode.FSImageFormatProtobuf: Image file /home/bpdp/software/big-data-dev-tools/hadoop-3.2.3/data-hdfs/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .
2022-05-23 18:04:54,722 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
2022-05-23 18:04:54,759 INFO namenode.FSNamesystem: Stopping services started for active state
2022-05-23 18:04:54,760 INFO namenode.FSNamesystem: Stopping services started for standby state
2022-05-23 18:04:54,762 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
2022-05-23 18:04:54,763 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at dellvuan/127.0.1.1
************************************************************/
$
```

Jalankan *daemon* untuk HDFS:

```bash
$ start-dfs.sh
Starting namenodes on [localhost]
Starting datanodes
Starting secondary namenodes [dellvuan]
$
```

Pada posisi ini, Apache Hadoop HDFS sudah siap digunakan untuk keperluan Apache Hive. Jika akan mematikan Apache Hadoop HDFS:

```bash
$ start-dfs.sh
Stopping namenodes on [localhost]
Stopping datanodes
Stopping secondary namenodes [dellvuan]
$
```

##  Instalasi Apache Hive


##  Struktur Direktori Apache Hive


##  Konfigurasi Apache Hive


##  Menjalankan Server Apache Hive: HiveServer2, HCatalog, WebHCatalog


##  Klien untuk Apache Hive


##  Model Data Apache Hive


##  SQL di Apache Hive: DDL, DML, DQL


## Kasus Sederhana

