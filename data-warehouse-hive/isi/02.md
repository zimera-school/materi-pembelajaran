# Memulai Apache Hive

##  Persyaratan Sistem

Apache Hadoop dan Apache Hive berjalan pada *commodity hardware* atau perangkat keras yang tidak spesifik khusus untuk server, dengan kata lain, perangkat keras yang biasa digunakan oleh pemakai biasa bisa digunakan untuk *stand alone* maupun *clustering*.

Apache Hive dan Apache Hadoop dibuat dengan menggunakan bahasa pemrograman Java sehingga JVM (Java Virtual Machine) diperlukan untuk mengoperasikan Apache Hive dan Apache Hadoop. Informasi tentang kebutuhan JVM untuk masing-masing bisa diperoleh di:

1.  [JVM untuk Apache Hadoop](https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions).
2.  [JVM untuk Apache Hive](https://cwiki.apache.org/confluence/display/Hive/AdminManual+Installation).

Perlu diketahui, dokumentasi untuk Apache Hive bukan merupakan dokumentasi terkini, sehingga untuk keperluan JVM sebaiknya menggunakan persyaratan dari Apache Hadoop:

1.  JDK 8 untuk Apache Hadoop 3.0.x - 3.2.x (mengkompilasi dan menjalankan)
2.  JDK 8 untuk mengkompilasi dan menjalankan Apache Hadoop 3.3.x, dan JRE 11 (*runtime* saja, tidak bisa untuk mengkompilasi kode sumber Apache Hadoop) untuk menjalankan Apache Hadoop 3.3.x
3.  Apache Hadoop yang digunakan adalah versi stabil (3.2.x)

JDK yang bisa digunakan adalah JDK dari [OpenJDK](https://adoptium.net/) maupun dari [Oracle](https://www.oracle.com/java/technologies/downloads/#java8). Versi dari vendor lain seharusnya bisa berjalan juga (misal, Amazon Corretto). 

##  Instalasi dan Konfigurasi Software Prasyarat

### JDK

Petunjuk instalasi untuk JDK:

1.  [Oracle JDK 8](https://docs.oracle.com/javase/8/docs/technotes/guides/install/install_overview.html).
2.  [Adoptium](https://adoptium.net/installation).

### Apache Hadoop

Untuk keperluan instalasi, siapkan direktori baru yang kosong. Kita akan menggunakan direktori ini sebagai dasar dari semua software yang kita gunakan:

```bash
$ mkdir -p $HOME/software/bigdata-dev-tools
$ mkdir -p $HOME/master
$ mkdir -p $HOME/env
$ mkdir -p $HOME/training
```

* Baris 1 digunakan sebagai tempat instalasi semua software yang digunakan
* Baris 2 digunakan untuu menyimpan semua master software
* Baris 3 digunakan untuk menyimpan semua file-file yang mengatur *environment variables*
* Baris 4 digunakan untuk menyimpan semua file yang dibuat saat training.

**Catatan:** Perintah-perintah yang dikerjakan di shell / CLI dilakukan pada kondisi keaktifan user tertentu. Jika diawali dengan **$**, maka perintah tersebut dikerjakan oleh user biasa. Jika diawali dengan **#**, maka perintah tersebut dikerjakan oleh *superuser* / *root*.

Setelah membuat direktori, masukkan semua file master di direktori `$HOME/master` kemudian masuk ke direktori tempat software akan di-install:

```bash
$ cd $HOME/software/bigdata-dev-tools
```

Pada posisi ini, semua direktori sudah tersedia dan semua software yang dibutuhkan sudah berada di `$HOME/master`. 


HDFS akan dibuat pada direktori `/tmp/hadoop-${user.name}`. Semua direktori dan file untuk data maupun metadata akan berada pada direktori tersebut. Jika akan mengubah, gunakan opsi **hadoop.tmp.dir** pada file `core-default.xml`. Informasi lengkap bisa dilihat di https://hadoop.apache.org/docs/r3.2.3/hadoop-project-dist/hadoop-common/core-default.xml

Direktori lain (untuk metadata serta data) menyesuaikan pada konfigurasi di atas. Jika ingin mengubah, konfigurasi bisa diatur di `hdfs-default.xml`. Informasi lengkap bisa dilihat pada https://hadoop.apache.org/docs/r3.2.3/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml. Untuk mengubah konfigurasi, atur:

1.  dfs.namenode.name.dir
2.  dfs.datanode.data.dir

Berikut ini adalah contoh konfigurasi di `core-default.xml`:

```xml
<configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/bpdp/software/big-data-dev-tools/hadoop-3.2.3/data-hdfs/</value>
    </property>
</configuration>
```

Untuk Apache Hadoop, diperlukan instalasi `ssh` dan `pdsh`. Install dengan menggunakan perintah:

```bash
$ sudo apt install ssh pdsh
```

Untuk instalasi Apache Hadoop, ekstrak distribusi Apache Hadoop yang sudah diunduh dan kemudian atur *environment variables*:

```bash
$ tar -xvf ~/master/hadoop-3.2.3.tar.g
...
...
...
...
...
hadoop-3.2.3/sbin/stop-balancer.sh
hadoop-3.2.3/sbin/workers.sh
hadoop-3.2.3/sbin/start-balancer.sh
hadoop-3.2.3/sbin/start-all.cmd
hadoop-3.2.3/sbin/hadoop-daemon.sh
hadoop-3.2.3/sbin/stop-yarn.sh
hadoop-3.2.3/sbin/start-all.sh
$ 
```

Setelah itu, buat file yang berisi *environment variables* dengan nama `$HOME/env/hadoop` sebagai berikut:

```bash
export HADOOP_HOME=$HOME/software/big-data-dev-tools/hadoop-3.2.3
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
export PDSH_RCMD_TYPE=ssh
```

Setelah itu, uji instalasi dengan:

```bash
$ source ~/env/hadoop
```

Jalankan Apache Hadoop:

```bash
$ hadoop
Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]
 or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]
  where CLASSNAME is a user-provided Java class

  OPTIONS is none or any of:

buildpaths                       attempt to add class files from build tree
--config dir                     Hadoop config directory
--debug                          turn on shell script debug mode
--help                           usage information
hostnames list[,of,host,names]   hosts to use in slave mode
hosts filename                   list of hosts to use in slave mode
loglevel level                   set the log4j level for this command
workers                          turn on worker mode

  SUBCOMMAND is one of:


    Admin Commands:

daemonlog     get/set the log level for each daemon

    Client Commands:

archive       create a Hadoop archive
checknative   check native Hadoop and compression libraries availability
classpath     prints the class path needed to get the Hadoop jar and the required libraries
conftest      validate configuration XML files
credential    interact with credential providers
distch        distributed metadata changer
distcp        copy file or directories recursively
dtutil        operations related to delegation tokens
envvars       display computed Hadoop environment variables
fs            run a generic filesystem user client
gridmix       submit a mix of synthetic job, modeling a profiled from production load
jar <jar>     run a jar file. NOTE: please use "yarn jar" to launch YARN applications, not this command.
jnipath       prints the java.library.path
kdiag         Diagnose Kerberos Problems
kerbname      show auth_to_local principal conversion
key           manage keys via the KeyProvider
rumenfolder   scale a rumen input trace
rumentrace    convert logs into a rumen trace
s3guard       manage metadata on S3
trace         view and modify Hadoop tracing settings
version       print the version

    Daemon Commands:

kms           run KMS, the Key Management Server

SUBCOMMAND may print help when invoked w/o parameters or with -h.
$
```

Apache Hadoop bisa dijelankan dalam 3 mode:

1.  *Standalone*
2.  *Pseudo-Distributed*
3.  *Fully-Distributed*

### Standalone

Pada mode ini, Hadoop biasanya hanya digunakan untuk proses kecil dan untuk keperluan eksperimen serta *debugging* saja. Dengan menggunakan mode ini, Hadoop (yang dibuat menggunakan Java) akan dijalankan dalam 1 proses Java saja.


Ubah konfigurasi Apache Hadoop untuk mode *pseudo-distributed*

_$HADOOP_HOME/etc/hadoop/core-site.xml_

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```

_$HADOOP_HOME/etc/hadoop/hdfs-site.xml_

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
```

_$HADOOP_HOME/etc/hadoop/hadoop-env.sh_

```bash
export JAVA_HOME=/home/bpdp/software/java-dev-tools/eclipse-temurin/temurin8
export HADOOP_HOME=/home/bpdp/software/big-data-dev-tools/hadoop-3.1.3
```

**Catatan**: lokasi JAVA_HOME dan HADOOP_HOME silakan disesuikan dengan lokasi instalasi JDK dan Apache Hadoop.

Setelah itu, atur supaya bisa login ssh tanpa *passphrase*:

```bash
$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
$ chmod 0600 ~/.ssh/authorized_keys
```

Uji dengan perintah berikut:


```bash
$ ssh localhost                                                            (py39-hive)
Linux dellvuan 5.17.0-2-amd64 #1 SMP PREEMPT Debian 5.17.6-1 (2022-05-14) x86_64

The programs included with the Devuan GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Devuan GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
You have new mail.
Last login: Mon May 23 16:43:39 2022
$
```

Setelah itu, format *filesystem*:

```bash
$ hdfs namenode -format 
WARNING: /home/bpdp/software/big-data-dev-tools/hadoop-stable/logs does not exist. Creating.
2022-05-23 17:53:39,064 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = dellvuan/127.0.1.1
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 3.2.3
STARTUP_MSG:   classpath = /home/bpdp/software/big-data-dev-tools/hadoop-stable/etc/hadoop:/home/bpdp/software/big-data-dev-tools/hadoop-stable/share/hadoop/common/lib/paranamer-2.3.jar...
...
...
...
STARTUP_MSG:   build = https://github.com/apache/hadoop -r abe5358143720085498613d399be3bbf01e0f131; compiled by 'ubuntu' on 2022-03-20T01:18Z
STARTUP_MSG:   java = 1.8.0_332
************************************************************/
2022-05-23 18:04:53,863 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2022-05-23 18:04:53,921 INFO namenode.NameNode: createNameNode [-format]
Formatting using clusterid: CID-adddc347-e302-4817-8019-ddcc12113b20
2022-05-23 18:04:54,227 INFO namenode.FSEditLog: Edit logging is async:true
2022-05-23 18:04:54,246 INFO namenode.FSNamesystem: KeyProvider: null
2022-05-23 18:04:54,247 INFO namenode.FSNamesystem: fsLock is fair: true
2022-05-23 18:04:54,249 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2022-05-23 18:04:54,253 INFO namenode.FSNamesystem: fsOwner             = bpdp (auth:SIMPLE)
2022-05-23 18:04:54,253 INFO namenode.FSNamesystem: supergroup          = supergroup
2022-05-23 18:04:54,253 INFO namenode.FSNamesystem: isPermissionEnabled = true
2022-05-23 18:04:54,255 INFO namenode.FSNamesystem: HA Enabled: false
2022-05-23 18:04:54,289 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2022-05-23 18:04:54,297 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2022-05-23 18:04:54,297 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2022-05-23 18:04:54,300 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2022-05-23 18:04:54,300 INFO blockmanagement.BlockManager: The block deletion will start around 2022 May 23 18:04:54
2022-05-23 18:04:54,301 INFO util.GSet: Computing capacity for map BlocksMap
2022-05-23 18:04:54,301 INFO util.GSet: VM type       = 64-bit
2022-05-23 18:04:54,302 INFO util.GSet: 2.0% max memory 1.7 GB = 34.3 MB
2022-05-23 18:04:54,303 INFO util.GSet: capacity      = 2^22 = 4194304 entries
2022-05-23 18:04:54,311 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
2022-05-23 18:04:54,311 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
2022-05-23 18:04:54,315 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2022-05-23 18:04:54,315 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2022-05-23 18:04:54,316 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2022-05-23 18:04:54,316 INFO blockmanagement.BlockManager: defaultReplication         = 1
2022-05-23 18:04:54,316 INFO blockmanagement.BlockManager: maxReplication             = 512
2022-05-23 18:04:54,316 INFO blockmanagement.BlockManager: minReplication             = 1
2022-05-23 18:04:54,316 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
2022-05-23 18:04:54,316 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2022-05-23 18:04:54,316 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
2022-05-23 18:04:54,316 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2022-05-23 18:04:54,333 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
2022-05-23 18:04:54,333 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
2022-05-23 18:04:54,333 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
2022-05-23 18:04:54,333 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
2022-05-23 18:04:54,362 INFO util.GSet: Computing capacity for map INodeMap
2022-05-23 18:04:54,362 INFO util.GSet: VM type       = 64-bit
2022-05-23 18:04:54,362 INFO util.GSet: 1.0% max memory 1.7 GB = 17.2 MB
2022-05-23 18:04:54,362 INFO util.GSet: capacity      = 2^21 = 2097152 entries
2022-05-23 18:04:54,363 INFO namenode.FSDirectory: ACLs enabled? false
2022-05-23 18:04:54,363 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
2022-05-23 18:04:54,363 INFO namenode.FSDirectory: XAttrs enabled? true
2022-05-23 18:04:54,363 INFO namenode.NameNode: Caching file names occurring more than 10 times
2022-05-23 18:04:54,367 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2022-05-23 18:04:54,369 INFO snapshot.SnapshotManager: SkipList is disabled
2022-05-23 18:04:54,373 INFO util.GSet: Computing capacity for map cachedBlocks
2022-05-23 18:04:54,373 INFO util.GSet: VM type       = 64-bit
2022-05-23 18:04:54,373 INFO util.GSet: 0.25% max memory 1.7 GB = 4.3 MB
2022-05-23 18:04:54,373 INFO util.GSet: capacity      = 2^19 = 524288 entries
2022-05-23 18:04:54,378 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2022-05-23 18:04:54,378 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2022-05-23 18:04:54,378 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2022-05-23 18:04:54,381 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
2022-05-23 18:04:54,381 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2022-05-23 18:04:54,382 INFO util.GSet: Computing capacity for map NameNodeRetryCache
2022-05-23 18:04:54,382 INFO util.GSet: VM type       = 64-bit
2022-05-23 18:04:54,382 INFO util.GSet: 0.029999999329447746% max memory 1.7 GB = 527.2 KB
2022-05-23 18:04:54,382 INFO util.GSet: capacity      = 2^16 = 65536 entries
2022-05-23 18:04:54,404 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1171203833-127.0.1.1-1653303894395
2022-05-23 18:04:54,517 INFO common.Storage: Storage directory /home/bpdp/software/big-data-dev-tools/hadoop-3.2.3/data-hdfs/dfs/name has been successfully formatted.
2022-05-23 18:04:54,580 INFO namenode.FSImageFormatProtobuf: Saving image file /home/bpdp/software/big-data-dev-tools/hadoop-3.2.3/data-hdfs/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression
2022-05-23 18:04:54,652 INFO namenode.FSImageFormatProtobuf: Image file /home/bpdp/software/big-data-dev-tools/hadoop-3.2.3/data-hdfs/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .
2022-05-23 18:04:54,722 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
2022-05-23 18:04:54,759 INFO namenode.FSNamesystem: Stopping services started for active state
2022-05-23 18:04:54,760 INFO namenode.FSNamesystem: Stopping services started for standby state
2022-05-23 18:04:54,762 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
2022-05-23 18:04:54,763 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at dellvuan/127.0.1.1
************************************************************/
$
```

Jalankan *daemon* untuk HDFS:

```bash
$ start-dfs.sh
Starting namenodes on [localhost]
Starting datanodes
Starting secondary namenodes [dellvuan]
$
```

Pada posisi ini, Apache Hadoop HDFS sudah siap digunakan untuk keperluan Apache Hive. Jika akan mematikan Apache Hadoop HDFS:

```bash
$ start-dfs.sh
Stopping namenodes on [localhost]
Stopping datanodes
Stopping secondary namenodes [dellvuan]
$
```

##  Instalasi Apache Hive

Apache Hive bisa diperoleh di https://hive.apache.org/. Instalasi dilakukan dengan cara meng-ekstrak software yang telah diunduh ke suatu lokasi serta mengatur berbagai *environment variables* dan konfigurasinya.

```bash
$ tar -xvf ~/master/apache-hive-3.1.3-bin.tar.gz
...
...
$
```

Setelah itu, buat file untuk *environement variabels* di `$HOME/env/hive-3`:

```bash
export HIVE_HOME /home/bpdp/software/big-data-dev-tools/hive-3
export PATH $HIVE_HOME/bin $HIVE_HOME/hcatalog/bin $HIVE_HOME/hcatalog/sbin $PATH
```

Setiap kali akan menggunakan Apache Hive di shell, kerjakan `source ~/env/hive-3` di shell yang baru tersebut.

##  Struktur Direktori Apache Hive

Berikut adalah struktur direktori dari Apache Hive.

```bash
$ tree -d .
.
├── bin
│   └── ext
│       └── util
├── binary-package-licenses
├── conf
├── examples
│   ├── files
│   └── queries
├── hcatalog
│   ├── bin
│   ├── etc
│   │   ├── hcatalog
│   │   └── webhcat
│   ├── libexec
│   ├── sbin
│   └── share
│       ├── doc
│       │   └── hcatalog
│       ├── hcatalog
│       └── webhcat
│           ├── java-client
│           └── svr
│               └── lib
├── jdbc
├── lib
│   ├── php
│   │   ├── ext
│   │   │   └── thrift_protocol
│   │   │       └── tags
│   │   │           └── 1.0.0
│   │   ├── packages
│   │   │   ├── fb303
│   │   │   ├── queryplan
│   │   │   └── serde
│   │   │       └── org
│   │   │           └── apache
│   │   │               └── hadoop
│   │   │                   └── hive
│   │   │                       └── serde
│   │   ├── protocol
│   │   └── transport
│   └── py
│       ├── fb303
│       ├── fb303_scripts
│       ├── hive_serde
│       ├── queryplan
│       └── thrift
│           ├── protocol
│           ├── reflection
│           │   └── limited
│           ├── server
│           └── transport
└── scripts
    ├── llap
    │   ├── bin
    │   ├── sql
    │   └── yarn
    └── metastore
        └── upgrade
            ├── derby
            ├── hive
            ├── mssql
            ├── mysql
            ├── oracle
            └── postgres

65 directories
$
```

1. Direktori `bin`: berisi berbagai file *executable* dari Apache Hive.
2. Direktori `binary-package-licenses`: berisi lisensi dari berbagai paket yang digunakan oleh Apache Hive.
3. Direktori `conf`: konfigurasi dari Apache Hive.
4. Direktori `examples`: berisi berbagai contoh *query* dan file yang bisa digunakan.
5. Direktiri `hcatalog`: berisi HCatalog. HCatalog adalah ekstensi dari Apache Hive yang digunakan untuk lapisan pengelolaan *storage* sehingga data bisa diakses secara mudah menggunakan peranti software lain (**Pig**, **MapReduce**).
6. Direktori `jdbc`: berisi drive JDBC untuk Java (dan bahasa pemrograman lain yang berjalan pada JVM - Kotlin, Scala, Clojure).
7. Direktori `lib`: berisi berbagai pustaka *jar* yang digunakan Apache Hive serta PHP dan Python.
8. Direktori `scripts`: berisi berbagai *script* yang diperlukan secara internal oleh Apache Hive.

##  Konfigurasi Apache Hive

Apache Hive menyimpan konfigurasi di direktori `$HIVE_HOME/conf`:

```bash
$ ls -la
total 340
drwxr-xr-x  2 bpdp bpdp   4096 May 12 05:47 ./
drwxr-xr-x 10 bpdp bpdp   4096 May 24 04:36 ../
-rw-r--r--  1 bpdp bpdp   1596 Oct 24  2019 beeline-log4j2.properties.template
-rw-r--r--  1 bpdp bpdp 300727 Apr  4 03:58 hive-default.xml.template
-rw-r--r--  1 bpdp bpdp   2365 Oct 24  2019 hive-env.sh.template
-rw-r--r--  1 bpdp bpdp   2274 Oct 24  2019 hive-exec-log4j2.properties.template
-rw-r--r--  1 bpdp bpdp   3086 Sep  5  2020 hive-log4j2.properties.template
-rw-r--r--  1 bpdp bpdp   2060 Oct 24  2019 ivysettings.xml
-rw-r--r--  1 bpdp bpdp   3558 Sep  5  2020 llap-cli-log4j2.properties.template
-rw-r--r--  1 bpdp bpdp   6937 Mar 29 02:39 llap-daemon-log4j2.properties.template
-rw-r--r--  1 bpdp bpdp   2662 Oct 24  2019 parquet-logging.properties
$
```

Dari berbagai file tersebut, bisa dibuat 1 file yang berisi konfigurasi untuk keseluruhan Apache Hive. Konfigurasi tersebut ada pada file `hive-site.xml`. Default konfigurasi terdapat pada file `hive-default.xml.template` (file ini tidak dibaca oleh Apache Hive dan hanya berfungsi untuk inventarisasi kofigurasi default). Jika ingin mengubah, *copy* konfigurasi dari template dan kemudian tulis pada `hive-site.xml`. File-file lain juga merupakan konfigurasi masing-masing dan bisa diatur.

Untuk keperluan menjalankan server Apache Hive dan peranti pendukungnya, siapkan direktori di Apache Hadoop (ingat `start-dfs.sh` sudah harus dijalankan lebih dulu). User untuk Apache Hive dibuat untuk OS tempat Apache Hive tersebut dijalankan (untuk keperluan ini, digunakan username:password **hive:hive**.

```bash
$ hadoop fs -mkdir /tmp
$ sudo adduser hive
Adding user `hive' ...
Adding new group `hive' (1004) ...
Adding new user `hive' (1002) with group `hive' ...
Creating home directory `/home/hive' ...
Copying files from `/etc/skel' ...
New password:
Retype new password:
passwd: password updated successfully
Changing the user information for hive
Enter the new value, or press ENTER for the default
	Full Name []: Apache Hive
	Room Number []:
	Work Phone []:
	Home Phone []:
	Other []:
Is the information correct? [Y/n] Y
$ hadoop fs -mkdir /user
$ hadoop fs -mkdir /user/hive
$ hadoop fs -mkdir /user/hive/warehouse
```

Atur hak akses:

```bash
$ hadoop fs -chmod g+w /tmp
$ hadoop fs -chmod g+w /user/hive/warehouse
```

Untuk menjalankan Apache Hive, perlu dilakukan inisialisasi terhadap *metastore*. Suatu *metastore* digunakan oleh Apache Hive untuk menyimpan informasi tentang berbagai data yang ada pada Apache Hive. Secara default, Apache Derby embedded sudah disediakan oleh Apache Hive, tetapi biasanya hanya untuk keperluan *testing* saja. Untuk pemakaian serius, biasanya menggunakan MySQL/MariaDB/PostgreSQL. Untuk keperluan ini, kita akan menggunakan MariaDB.

Instalasi MariaDB ada di luar ruang lingkup pelatihan ini. Gunakan informasi yang ada pada https://mariadb.com/kb/en/getting-installing-and-upgrading-mariadb/ untuk instalasi sesuai dengan sistem operasi yang digunakan. Catat juga username:password untuk administrator (root) di MariaDB sesuai konfigurasi anda. Setelah itu ambil driver MariaDB untuk Java di https://mvnrepository.com/artifact/org.mariadb.jdbc/mariadb-java-client. Ambil file .jar dan masukkan ke $HIVE_HOME/lib:


```bash
$ pwd
/home/bpdp/software/big-data-dev-tools/apache-hive-3.1.3-bin/lib
$ ls -la 
...
...
-rw-r--r-- 1 bpdp bpdp 571732 May 24 05:38 mariadb-java-client-3.0.4.jar
...
...
$
```

Atur konfigurasi di `hive-site.xml`:

```xml
<configuration>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mariadb://localhost:3306/metastore?createDatabaseIfNotExist=true</value>
        <description>metadata is stored in a MariaDB server</description>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>org.mariadb.jdbc.Driver</value>
        <description>MariaDB JDBC driver class</description>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
        <description>user name for connecting to mariadb server</description>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>root</value>
        <description>password for connecting to mariadb server</description>
    </property>
</configuration>
```
Setelah itu inisialisasi Schema menggunakan perintah berikut (perintah ini hanya sekali saja, untuk persiapan menggunakan Apache Hive):

```bash
schematool -dbType mysql -initSchema
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/bpdp/software/big-data-dev-tools/apache-hive-3.1.3-bin/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/bpdp/software/big-data-dev-tools/hadoop-3.2.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Metastore connection URL:	 jdbc:mysql://localhost:3306/metastore?createDatabaseIfNotExist=true
Metastore Connection Driver :	 com.mysql.jdbc.Driver
Metastore connection User:	 root
Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.
Starting metastore schema initialization to 3.1.0
Initialization script hive-schema-3.1.0.mysql.sql

Initialization script completed
schemaTool completed
```

**PENTING**

Saat menjalankan schematool atau berbagai *executable* di Apache Hive, ada kemungkinan muncul muncul error: 

```
$ schematool -dbType mysql -initSchema
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/bpdp/software/big-data-dev-tools/apache-hive-3.1.3-bin/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/bpdp/software/big-data-dev-tools/hadoop-3.2.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Exception in thread "main" java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)
	at org.apache.hadoop.mapred.JobConf.setJar(JobConf.java:536)
	at org.apache.hadoop.mapred.JobConf.setJarByClass(JobConf.java:554)
	at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:448)
	at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:5144)
	at org.apache.hadoop.hive.conf.HiveConf.<init>(HiveConf.java:5107)
	at org.apache.hive.beeline.HiveSchemaTool.<init>(HiveSchemaTool.java:96)
	at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:1473)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:236)
```

Error tersebut terjadi karena versi `guava` pada Apache Hadoop dan Apache Hive berbeda. Lihat pada `$HIVE_HOME/lib` dan `$HADOOP_HOME/share/share/hadoop/common/lib/`. Kedua file `guava` harus mempunyai versi yang sama. Ambil versi yang paling baru di antara keduanya.

Jika sudah sampai pada titik ini, berarti sudah siap menggunakan Apache Hive.

Konfigurasi terakhir untuk `hive-site.xml`:

```xml
<configuration>
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://localhost:3306/metastore?createDatabaseIfNotExist=true</value>
    <description>metadata is stored in a MySQL server</description>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.cj.jdbc.Driver</value>
    <description>MySQL JDBC driver class</description>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>root</value>
    <description>user name for connecting to MySQL server</description>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>root</value>
    <description>password for connecting to MySQL server</description>
  </property>
  <property>
    <name>hive.server2.enable.doAs</name>
    <value>false</value>
    <description>
      Setting this property to true will have HiveServer2 execute
      Hive operations as the user making the calls to it.
    </description>
  </property>
</configuration>
```

##  Menjalankan Server Apache Hive: HiveServer2, HCatalog, WebHCatalog

Untuk menjalankan *HiveServer2*:

```bash
$ hiveserver2
2022-05-25 16:04:09: Starting HiveServer2
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/bpdp/software/big-data-dev-tools/apache-hive-3.1.3-bin/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/bpdp/software/big-data-dev-tools/hadoop-3.2.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = d25f9ef7-ea79-43cb-9d5e-ba3a2df5d0ad
Hive Session ID = 0ab93999-9205-4cf9-87bf-3cd6eee872e1
```

##  Klien untuk Apache Hive

Apache Hive yang telah dijalankan di atas bisa diakses melalui **Beeline** yang berfungsi sebagai *shell client*:

```bash
$ beeline -u jdbc:hive2://localhost:10000
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/bpdp/software/big-data-dev-tools/apache-hive-3.1.3-bin/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/bpdp/software/big-data-dev-tools/hadoop-3.2.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Connecting to jdbc:hive2://localhost:10000
Connected to: Apache Hive (version 3.1.3)
Driver: Hive JDBC (version 3.1.3)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 3.1.3 by Apache Hive
0: jdbc:hive2://localhost:10000>
```


##  Model Data Apache Hive


##  SQL di Apache Hive: DDL, DML, DQL


## Kasus Sederhana

